{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow.kernels import White, RBF\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from deep_gp import DeepGP\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Gaussian Processes\n",
    "\n",
    "-------------------\n",
    "\n",
    "# Part 1 - Sampling From a Deep GP (worth 1 point)\n",
    "\n",
    "#### You are provided with the following ingredients with which to build a GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(111)\n",
    "N = 500\n",
    "x = np.linspace(-20, 20, num=N)\n",
    "\n",
    "gp_var = 1.0  # GP variance\n",
    "gp_len = 1.0  # GP lengthscale\n",
    "\n",
    "def rbf_kernel(x1, x2):\n",
    "    x1 = x1.reshape(-1, 1, 1)\n",
    "    x2 = x2.reshape(1, -1, 1)\n",
    "    r = np.sum(x1 - x2, axis=-1)\n",
    "    return gp_var * np.exp(-0.5 * np.square(r) / np.square(gp_len))\n",
    "\n",
    "\n",
    "def zero_mean(X):\n",
    "    return np.zeros_like(X.reshape(-1, ))\n",
    "\n",
    "\n",
    "def identity_mean(X):\n",
    "    return X.reshape(-1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1a**: Generate a sample from a 3-layer deep GP with **zero mean** and an **RBF kernel**. Use the inputs, x, and parameters given above. Plot the sample path and the covariance matrix in each layer.\n",
    "\n",
    "**Do you notice any strange behaviour in the sample path and/or covariance matrix? Write a short explanation of the behaviour and why you think it may be occuring.**\n",
    "\n",
    "hint 1: you can use np.random.multivariate_normal() to sample from a Gaussian distribution. ***Note: you may need to add a small value to the diagonal of the covariance matrix to prevent numerical issues during sampling, e.g. + 1e-10 * np.eye(N)***\n",
    "\n",
    "hint 2: use plt.imshow() to plot a covariance matrix\n",
    "\n",
    "hint 3: always plot the initial inputs, x, on the x-axis of the sample plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1b**: Now generate a sample from a 3-layer deep GP, now with the **identity mean function for the first two layers**, and a **zero mean function for the final layer**. Plot the samples from each layer and the covariance matrices.\n",
    "\n",
    "**Do you notice any differences in the behaviour between this sample and the previous one? Write a short explanation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "# Part 2 - Training a Deep GP (worth 1 point)\n",
    "\n",
    "Here we generate some training data using the step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "np.random.seed(123)\n",
    "N = 40\n",
    "X_train = np.linspace(-1, 1, N)[:, None]\n",
    "f_step = lambda x: -1 if x < 0 else 1.\n",
    "Y_train = np.reshape([f_step(x) for x in X_train], X_train.shape) + np.random.randn(*X_train.shape) * 1e-2\n",
    "Xs = np.linspace(-2., 2, 300)[:, None]  # test inputs\n",
    "plt.scatter(X_train, Y_train, color='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2a:** The ELBO calculation has been removed from the deep GP implementation. **Implement the ELBO yourself using the derivation on slide 16 of the lecture.**\n",
    "\n",
    "A custom deep GP class has been created below. You are only required to finish the implementation of the ELBO, using the ingredients given in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDeepGP(DeepGP):\n",
    "\n",
    "    def elbo(self, data):\n",
    "        \"\"\"\n",
    "        Implement the ELBO here using the derivation on slide 16 of the lecture notes\n",
    "        :param data: Tuple of two tensors for input data X and labels Y.\n",
    "        :return: Tensor representing ELBO.\n",
    "        \"\"\"\n",
    "        X, Y = data  # mini-batch data for the current training step\n",
    "        N_star = X.shape[0]  # number of points in the mini-batch\n",
    "        N = self.num_data  # total number of data points\n",
    "        S = self.num_samples  # the number of samples used to evaluate the marginals, q(f_{L,n})\n",
    "        \n",
    "        ############################################\n",
    "        ######### Implement the ELBO here ##########\n",
    "        ############################################\n",
    "        \n",
    "        # You will need the following ingredients:\n",
    "        \n",
    "        # self.predict_f() produces samples from the marginal, q(f_{L,n,s}):\n",
    "        F_mean, F_var = self.predict_f(X, num_samples=S, full_cov=False)\n",
    "        \n",
    "        # now use self.likelihood.variational_expectations(F_mean, F_var, Y) to evaluate following:\n",
    "        # \\int q(f_{L,n,s}) ln p(y_n | f_{L,n,s}) d f_{L,n,s}\n",
    "\n",
    "        # Compute the KL term. The KL for a single layer, i, can be computed as follows:\n",
    "        # gpflow.kullback_leiblers.gauss_kl(self.layers[i].q_mu, self.layers[i].q_sqrt, K=self.layers[i].Ku)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model and plot the posterior by running the following code (no modification needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions used to build and train the deep GP\n",
    "\n",
    "def make_deep_GP(num_layers, X, Y, Z):\n",
    "    kernels = []\n",
    "    layer_sizes = []\n",
    "    for l in range(num_layers):\n",
    "        kernel = RBF(lengthscales=0.2, variance=1.0) + White(variance=1e-4)\n",
    "        kernels.append(kernel)\n",
    "        layer_sizes.append(1)\n",
    "\n",
    "    dgp = MyDeepGP(X, Y, Z, kernels, layer_sizes, Gaussian(variance=1), num_samples=20)\n",
    "\n",
    "    # init hidden layers to be near deterministic\n",
    "    for layer in dgp.layers[:-1]:\n",
    "        layer.q_sqrt.assign(layer.q_sqrt * 1e-2)\n",
    "    return dgp\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def training_step(model, opt):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        obj = -model.elbo((X_train, Y_train))\n",
    "        gradients = tape.gradient(obj, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Now train the model and plot the result\n",
    "\n",
    "M = 20  # num inducing points\n",
    "num_layers = 3  # number of layers\n",
    "Z_init = np.random.uniform(-1, 1, M)[:, None]\n",
    "dgp = make_deep_GP(num_layers, X_train, Y_train, Z_init)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01, epsilon=1e-08)\n",
    "\n",
    "num_iters = 5000\n",
    "\n",
    "for it in range(1, num_iters + 1):\n",
    "    objective = training_step(dgp, optimizer)\n",
    "    if np.mod(it, 100) == 0:\n",
    "        print('iter %2d, ELBO: %1.4f' % (it, -objective.numpy()))\n",
    "\n",
    "m_test, v_test = dgp.predict_y(Xs, num_samples=1000)\n",
    "m_test, v_test = tf.reduce_mean(m_test, axis=0), tf.reduce_mean(v_test, axis=0)\n",
    "lb = m_test[:, 0] - 1.96 * v_test[:, 0] ** 0.5\n",
    "ub = m_test[:, 0] + 1.96 * v_test[:, 0] ** 0.5\n",
    "\n",
    "plt.plot(Xs, m_test, color='b', alpha=0.3)\n",
    "plt.fill_between(Xs[:, 0], lb, ub, color='b', alpha=0.05, label='95% confidence')\n",
    "plt.xlim(min(Xs), max(Xs))\n",
    "\n",
    "plt.title('Step Function: Deep GP')\n",
    "plt.scatter(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2b:** Plot the model output from each layer. **Write a brief explanation of what role you think each layer is playing in inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction in the deep GP is also done via sampling \n",
    "# The following code produces 50 predictive samples from each layer.\n",
    "_, layers_means, layers_variances = dgp.predict_all_layers(Xs, num_samples=50, full_cov=False)\n",
    "# You can get a good estimate of the posterior predictive mean and variance by taking the mean of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model inference of lecture 9 (worth 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Pick some small data set (or a subset of a big data set). Study a GP model of your own choosing, where the latent values can't be integrated out analytically. \n",
    "\n",
    "Examples (See Lecture 9):\n",
    "- normal(mu,exp(eta)), mu ~ GP, eta ~ GP\n",
    "  e.g. https://avehtari.github.io/casestudies/Motorcycle/motorcycle.html\n",
    "- increasing magnitude model\n",
    "  e.g. https://avehtari.github.io/casestudies/Birthdays/birthdays.html\n",
    "- non-normal observation model, like Bernoulli, Poisson or negative-binomial\n",
    "  e.g. http://avehtari.github.io/BDA_R_demos/demos_rstan/trafficdeaths.html\n",
    "\n",
    "You may use any software you like (Stan, gpflow, gpytorch), and you may use full MCMC or\n",
    "integrate over latent values using Laplace, EP or VI.\n",
    "\n",
    "**Write a short report**:\n",
    "1. describe your model and approach\n",
    "2. what are optimized parameters vs. posterior of parameters\n",
    "3. show predictive distribution with optimization vs. posterior predictive\n",
    "\n",
    "For those familiar with GPflow, see\n",
    "https://gpflow.readthedocs.io/en/develop/notebooks/advanced/mcmc.html#Example-3:-Fully-Bayesian-inference-for-generalized-GP-models-with-HMC\n",
    "\n",
    "For those familiar with PyMC3, see\n",
    "https://docs.pymc.io/notebooks/GP-Latent.html\n",
    "\n",
    "**Hint**:\n",
    "- Check the Lecture 9 video\n",
    "- Studying the motorcycle casestudy is a good starting point\n",
    "- If you have questions, check the course Slack channel https://join.slack.com/t/gp2021e4075/shared_invite/zt-l0x4rfc5-go1vZngUsvq9ix9wdMQAIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
